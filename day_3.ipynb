{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3\n",
    "\n",
    "November 11, 2015\n",
    "\n",
    "## Machine Learning with scikit-learn\n",
    "\n",
    "[Resources](https://github.com/amueller/odscon-2015)\n",
    "\n",
    "Today mostly on supervised ML: training and generalisation\n",
    "\n",
    "### Using scikit-learn\n",
    "\n",
    "#### Supervised\n",
    "\n",
    "1.  import model class\n",
    "2.  instantiate classifier\n",
    "2.  use `classifier.fit()` function to build\n",
    "3.  use `classifier.predict(X_test)`\n",
    "4.  use `classfier.score(X_test, y_test)`\n",
    "\n",
    "rows are examples, columns are features, need to be numpy array, most of scikit-learn requires numeric data\n",
    "\n",
    "#### Unsupervised\n",
    "\n",
    "e.g.. remove mean, normalise variance\n",
    "\n",
    "1. import and instantiate\n",
    "2. fit\n",
    "3. transform\n",
    "\n",
    "is there an inverse transform? yes. use `inverse_transform()`: scaling, PCA, matrix decomposition all have inverses\n",
    "\n",
    "#### cross validation\n",
    "\n",
    "`cross_val_score(classifier, X, y, scoring=\"some_scoring_method\")`\n",
    "\n",
    "imbalanced classes: use `scoring=\"roc_auc\"`\n",
    "\n",
    "Shuffle split cross-validation: shuffles data and then splits into learning and test sets\n",
    "\n",
    "set `n_jobs` to parallelise\n",
    "\n",
    "#### parameter tuning\n",
    "\n",
    "use grid search (cross validation)\n",
    "\n",
    "`GridSearchCV`: gird search with automatic cross-validation\n",
    "\n",
    "1.  define parameter grid as dictionary: keys are parameter names, values are values to search over\n",
    "2.  `GridSearchCV(classifier(), param_grid, verbose=3)`\n",
    "    *   `cv=`\n",
    "3.  `.score()`\n",
    "\n",
    "DON'T JUST CROSS-VALIDATE YOUR CLASSIFIER; CROSS-VALIDATE YOUR ENTIRE **PIPELINE**\n",
    "\n",
    "#### pipelines\n",
    "\n",
    "1.  `pipeline = make_pipeline()`\n",
    "    *   takes transformations\n",
    "2.  `pipeline.fit()`\n",
    "3.  `pipeline.score()`\n",
    "4.  `pipeline.predict()`\n",
    "\n",
    "you can chuck an entire pipeline into `cross_val_score()`!\n",
    "\n",
    "can also use grid-search with a pipeline\n",
    "\n",
    "*   caveat: need to specify at which step in the pipeline you want to vary the params\n",
    "*   change key in the params dictionary from `C` to `svc__C` to specify\n",
    "\n",
    "### bag of words\n",
    "\n",
    "1.  get string\n",
    "2.  tokenise\n",
    "3.  build vocabulary\n",
    "4.  sparse matrix encoding of word frequencies over entire dictionary (superset of the built vocabulary)\n",
    "\n",
    "#### application: insult detection\n",
    "\n",
    "1.  read in data with pandas\n",
    "2.  `import CountVectorizer` and instantiate\n",
    "3.  fit and score\n",
    "\n",
    "vectoriser can do ngrams (pairs/triples/n-ples of words)\n",
    "\n",
    "#### advanced pipelines\n",
    "\n",
    "feature union: takes in data, run multiple pre-processes in parallel, and then combines end results to pipe to main process\n",
    "\n",
    "*   e.g. combine CountVectorizer with `char` and `word`\n",
    "\n",
    "to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ 'featureunion__process1__param' : [...], 'featureunion__process1__param' : [...], ... }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### out of core learning\n",
    "\n",
    "single machine, bigger than your RAM\n",
    "\n",
    "don't do it. just get a better machine\n",
    "\n",
    "NYU is hiring research engineers???\n",
    "\n",
    "\n",
    "## d3 in jupyter\n",
    "\n",
    "https://github.com/stitchfix/d3-jupyter-tutorial\n",
    "\n",
    "sigmaJS: graphs\n",
    "\n",
    "don't do this all the time\n",
    "\n",
    "matplotlib, mpld3, bokeh, seaborn, toyplot, vispy, vincent...\n",
    "\n",
    "d3 for the \"moneygraph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"[insert HTML code here and it'll be inserted in the DOM]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d3\n",
    "\n",
    "1.  select DOM element array\n",
    "2.  attach an array of data to DOM element array\n",
    "3.  perform some actions for each array in the data array\n",
    "\n",
    "## pandas\n",
    "\n",
    "https://github.com/jreback/pydatanyc2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do method chaining Ã  la JS\n",
    "\n",
    "### date time accessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and co\n",
    "\n",
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select data by type\n",
    "\n",
    "### categoricals\n",
    "\n",
    "store stuff as categoricals to efficiently use space (~hashing)\n",
    "\n",
    "### boolean indexing\n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df( df.key < 0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can use `df.loc()` to filter, then retreive only certain columns\n",
    "\n",
    "### isin\n",
    "\n",
    "select only rows where a (categorical) column contains a given string\n",
    "\n",
    "rise: jupyter presentation tool\n",
    "\n",
    "### .counts()\n",
    "\n",
    "ranks rows by value of a column\n",
    "\n",
    "### positional indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)          # kinda like R's head and tail\n",
    "df.tail(5)\n",
    "\n",
    "df.loc()            # get by string or value or something\n",
    "df.iloc([2,5,10])   # get by index\n",
    "df.ix()             # combination of integer and value based indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hierarchical indexing\n",
    "\n",
    "*   index by multiple columns\n",
    "*   efficient lookups\n",
    "*   can do multi-axis indexing\n",
    "    *   have to specify all axes\n",
    "\n",
    "`.query()`: use SQL-like syntax\n",
    "\n",
    "### grouping\n",
    "\n",
    "1.  split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.get_group('column_name')[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  apply (reduction(`agg`)/transformation(`transform`)/apply(`apply`))\n",
    "    *   transform: reduce by group\n",
    "3.  combine\n",
    "\n",
    "### reshape\n",
    "\n",
    "`.stack` take column, shift to index\n",
    "\n",
    "`.unstack` inverse of stack\n",
    "\n",
    "can groupby multi-index\n",
    "\n",
    "can groupby multiple columns\n",
    "\n",
    "### tidy form\n",
    "\n",
    "*   row are observations\n",
    "*   columns are features\n",
    "*   each table is for one type of thing\n",
    "\n",
    "`assign()` duplicates a column, performs some operation on it\n",
    "\n",
    "from R: `melt()` and `pivot()` are things too \n",
    "\n",
    "`pipe()` and `map()`\n",
    "\n",
    "*   `pipe()` sends result of previous operation to function in args. basically a callback\n",
    "*   does an internal groupby\n",
    "\n",
    "### resampling\n",
    "\n",
    "`.resample()` groupby using time\n",
    "\n",
    "`.Grouper()` do multiple groupbys simultaneously\n",
    "\n",
    "### frequency\n",
    "\n",
    "frequency objects\n",
    "\n",
    "`.date_range()`\n",
    "\n",
    "### timezones\n",
    "\n",
    "`.tz_convert()`\n",
    "\n",
    "### time deltas\n",
    "\n",
    "subtraction of dates\n",
    "\n",
    "### missing values\n",
    "\n",
    "`.ffill()`\n",
    "\n",
    "can also interpolate\n",
    "\n",
    "### in the works\n",
    "\n",
    "rolling mean\n",
    "\n",
    "### compute\n",
    "\n",
    "can interact with other libraries\n",
    "\n",
    "theano, sympy, statsmodels, scikit-learn, numba, cython, dask\n",
    "\n",
    "`.unpack_grid_scores()` transform weird flat arrays back into something you can do shit with\n",
    "\n",
    "## Computational Biology\n",
    "\n",
    "Illumina next generation genome sequencing\n",
    "\n",
    "Genome not random\n",
    "\n",
    "$\\exists$ repeats. if >100 bases long, can't sequencing genome properly\n",
    "\n",
    "Long-read DNA sequencing in development\n",
    "\n",
    "Using Python to study cancer\n",
    "\n",
    "split-read variant calling (places where chromosomes got stuck together wrong; gene fusion)\n",
    "\n",
    "complex gene fusions\n",
    "\n",
    "SplitThreader: python graph library for representing rearranged genomes\n",
    "\n",
    "\n",
    "## Finding $\\lambda$ for ridge regression\n",
    "\n",
    "https://github.com/kathrynthegreat/InverseProblem\n",
    "\n",
    "hilbert matrices\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "cancer immunotherapy\n",
    "\n",
    "conditional random fields\n",
    "\n",
    "### RNNs\n",
    "\n",
    "input: outside source + own output from previous time step\n",
    "\n",
    "cons: vanishing and exploding gradients\n",
    "\n",
    "### LSTM\n",
    "\n",
    "keras\n",
    "\n",
    "theano, lasagne, chainer\n",
    "\n",
    "applications: generate fairy tales, translate languages, captioning\n",
    "\n",
    "## Dask\n",
    "\n",
    "http://blaze.pydata.org/blog/2015/09/16/reddit-impala/\n",
    "\n",
    "binder\n",
    "\n",
    "gitter\n",
    "\n",
    "## Linear Optimisation\n",
    "\n",
    "where to go on vacation? how to construct a reading list?\n",
    "\n",
    "simplex\n",
    "\n",
    "top 10 algorithms of 20th century\n",
    "\n",
    "*   routing & logistics\n",
    "*   financial planning\n",
    "*   manufacturing\n",
    "*   product mix\n",
    "\n",
    "in python\n",
    "\n",
    "*   solver\n",
    "*   modelling framework\n",
    "\n",
    "SciPy\n",
    "PyOMO\n",
    "PuLP\n",
    "\n",
    "## cycler in matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cycler"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
