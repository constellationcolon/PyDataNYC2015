{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2\n",
    "\n",
    "## Density-based clustering in Python\n",
    "\n",
    "*   k-means not always the best method\n",
    "    *   default, but not best\n",
    "    *   simple, resources available, implementations available, scaleable\n",
    "    *   assumes hyperspheres, may be hard to choose k\n",
    "*   Use density-based clustering\n",
    "    *   unknown underlying PDF\n",
    "    *   premise: estimate PDF\n",
    "    *   want to: threshold PDF and take **upper level set** and find the **connected components** of the upper level set\n",
    "    *   intersect datapoints with clusters, label using clusters\n",
    "    *   **pros** complex cluster shapes, don't need to know k, automatically find outliers\n",
    "    *   **cons** not as scalable, need distance metric, connected components computation expensive, not great for high dimensional data\n",
    "    *   popular: DBSCAN\n",
    "        *   partition data into core, boundary, noise groups\n",
    "        *   connect core points into clusters, associate with boundary points\n",
    "        *   scikit-learn\n",
    "            *   params\n",
    "                *   eps: neighbourhood radius\n",
    "                *   min_samples: min naighbours to form a cluster\n",
    "            *   can use any distance function you want\n",
    "        *   also in GraphLab (Dato)\n",
    "    *   applications: document deduplication\n",
    "        *   creates dictionary with word count\n",
    "        *   use jaccard distance, not euclidean\n",
    "*   Use level set trees\n",
    "    *   how to choose density level in DBSCAN?\n",
    "    *   want to change denstiy level $\\implies$ start from scratch\n",
    "    *   create tree once, pick your density later by just querying the tree\n",
    "    *   visualisation of high dim data\n",
    "    *   how to build?\n",
    "        *   estimate density at each point\n",
    "        *   construct similarity graph on the data: edges are points, vertices connect near neighbours (kNN)\n",
    "        *   keep track of components between graphs\n",
    "        *   remove lowest density points and their vertices until you get a split\n",
    "        *   identify outliers by taking the lowest 5% (density) points\n",
    "        *   shine with complex data (e.g. hurricane tracks)\n",
    "        *   DeBaCl\n",
    "\n",
    "\n",
    "## Data Workflows\n",
    "\n",
    "* reproducibility\n",
    "    * spectrum: publication only $\\iff$ full replication\n",
    "    * transparency\n",
    "    * don't want to rewrite code\n",
    "    * productising insights\n",
    "* workflows\n",
    "    * define separate process\n",
    "    * high level abstraction of the underlying processes\n",
    "    * types\n",
    "        * environment/purpose specific **pipelines**\n",
    "            * e.g scikit-learn's pipeline; a little like chaining callbacks\n",
    "            * transparency of model\n",
    "            * easily rework pipeline; configurable building blocks\n",
    "        * cross-environment processing pipelines\n",
    "            * luigi\n",
    "                * requires output file at each step\n",
    "                * each task (is a class)\n",
    "                    * params\n",
    "                    * dependencies\n",
    "                    * do stuff\n",
    "                    * output\n",
    "            * airflow\n",
    "                * explicit dependencies\n",
    "                * scheduling\n",
    "        * REPL notebook environments (e.g. jupyter)\n",
    "            * good for small data\n",
    "* workflow engines\n",
    "\n",
    "\n",
    "## Understanding Probabilistic Topic Models By Simulation\n",
    "\n",
    "*   how to cluster data?\n",
    "*   k means?\n",
    "*   gaussian mixture models?\n",
    "*   forward sampling\n",
    "*   bayesian style?\n",
    "    *   model as dirichlet distribution (multinomial distribution)\n",
    "*   generative process for distributions\n",
    "*   given data\n",
    "    *   find cluster for each point\n",
    "    *   find params of each cluster\n",
    "    *   find mixture proportion\n",
    "*   discrete mixture model\n",
    "    *   e.g. flip 3 sided weighted coin\n",
    "*   topics as distributions over words\n",
    "*   reverse sampling\n",
    "    *   given collection of documents\n",
    "        *   find topic for each document\n",
    "        *   find distribution over topics\n",
    "        *   find distribution over terms for each topic\n",
    "    *   cons\n",
    "        *   assuming word order irrelevant\n",
    "        *   single topc/document\n",
    "        *   assume number of topics known\n",
    "        *   assumes topics are uncorrelated\n",
    "*   LDA\n",
    "    *   can tell when a word with two meanings might belong to different topics\n",
    "    *   HDPLDA (hierarchical Dirichlet process LDA)\n",
    "    *   read Blei's LDA paper (available on ACM website)\n",
    "    *   pyLDAvis\n",
    "    *   paper by Griffiths and Steyvers\n",
    "    *   Applications\n",
    "        *   LDA for genetic sequences\n",
    "\n",
    "## Parallelising Python\n",
    "\n",
    "Multiprocessing in python\n",
    "\n",
    "joblib, multiprocessing\n",
    "\n",
    "*   easy to //\n",
    "    *   x-validation\n",
    "    *   grid search\n",
    "    *   random forest\n",
    "    *   kernel density\n",
    "\n",
    "GPU accelerated\n",
    "\n",
    "*   PyCUDA\n",
    "\n",
    "Neural net: Lasagne"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
